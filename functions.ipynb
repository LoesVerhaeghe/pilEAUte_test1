{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "455fdbb4",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf976f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_sensor(variable,start_date,end_date):\n",
    "    '''\n",
    "    Gets sensor data, expects as input the file, the start and end date\n",
    "    :variable: string\n",
    "    :start_date, end_date: string format YYYY-MM-DD HH:MM:SS\n",
    "    :return: pandas dataframe\n",
    "    '''\n",
    "    current_dir = os.path.join(directory) \n",
    "    \n",
    "    print(current_dir)\n",
    "    print(os.listdir(current_dir))\n",
    "    \n",
    "\n",
    "    for file in os.listdir(current_dir):\n",
    "        filename = os.fsdecode(file)\n",
    "        if filename == variable + r\".csv\":\n",
    "            df_data = pd.read_csv(os.fsdecode(os.path.join(current_dir, file)), sep=\";\")\n",
    "            df_data = df_data.set_index('Time')                        \n",
    "            df_data.index = pd.to_datetime(df_data.index, utc=False)\n",
    "            df_data_out = df_data[start_date:end_date]\n",
    "    \n",
    "    return df_data_out\n",
    "\n",
    "\n",
    "def get_outliers_dataset(variable,start_date,end_date):\n",
    "    \n",
    "    '''\n",
    "    Gets validated data set (real outliers), expects as input the variable, site name, the start and end date\n",
    "    :param site: string\n",
    "    :param start_date: string format YYYY-MM-DD HH:MM:SS\n",
    "    :return: pandas dataframe\n",
    "    '''    \n",
    "    current_dir = os.path.join(directory)\n",
    "    \n",
    "    for file in os.listdir(current_dir):\n",
    "        filename = os.fsdecode(file)\n",
    "        if filename == variable + r\".csv\":\n",
    "            df_data = pd.read_csv(os.fsdecode(os.path.join(current_dir, file)), sep=\";\", names=[\"Time\", \"Value\"])\n",
    "                       \n",
    "            df_data.replace(r'\\n', '', regex = True, inplace = True)                                  \n",
    "            df_data = df_data.set_index('Time')\n",
    "            \n",
    "            df_data.index = pd.to_datetime(df_data.index, utc=False)\n",
    "            \n",
    "            #remove duplicated values\n",
    "            df_data = df_data[~df_data.index.duplicated(keep='first')]\n",
    "            \n",
    "            df_data = df_data.sort_index()\n",
    "            \n",
    "            #df_data = df_data.set_index('Time')\n",
    "            #df_data.index = pd.to_datetime(df_data.index, utc=False)            \n",
    "                       \n",
    "    return df_data\n",
    "\n",
    "\n",
    "def Shewhart(points,window = 0, nstd = 3):\n",
    "    \n",
    "    #window_size --> size of moving window\n",
    "    #nstd -->  #number of standard deviations to consider for the boundaries\n",
    "                \n",
    "    if window !=0:\n",
    "        mean = points.rolling(window,min_periods=window // 2,center=True).mean()\n",
    "        std = points.rolling(window,min_periods=window // 2,center=True).std()\n",
    "        \n",
    "    else:\n",
    "        meanX = points.mean()\n",
    "        stdX = points.std()               \n",
    "        mean = points.copy()\n",
    "        std = points.copy()\n",
    "        mean.iloc[:] = meanX.values[0]       \n",
    "        std.iloc[:] = stdX.values[0]         \n",
    "        \n",
    "    std.iloc[0] = 0 #the first value turns into NaN because of no data\n",
    "            \n",
    "    mean_plus_std = mean + nstd*std\n",
    "    mean_minus_std = mean - nstd*std        \n",
    "             \n",
    "    is_outlier = (points.values > mean_plus_std) | (points.values < mean_minus_std)\n",
    "    outliers = points[is_outlier]\n",
    "              \n",
    "    return outliers, mean, mean_plus_std,mean_minus_std, is_outlier\n",
    "    \n",
    "\n",
    "def rolling_IQR(points,window = 0,t0 = 1.5):\n",
    "    '''\n",
    "    Classifies a point as an outlier using the Inter quartile range from a boxplot. If it deviates\n",
    "    a set amount of IQR from the median it's classified as an outlier\n",
    "    :param points: pandas series\n",
    "    :param window: \n",
    "    :param min_periods: Minimum number of observations in window required to have a value (otherwise result is NA).\n",
    "    :param t0: threshold value: industry standard is 1.5 but it can go until 3\n",
    "    '''\n",
    "    if window != 0:\n",
    "        q1 = points.rolling(window,min_periods=window//2, center=True).quantile(0.25)\n",
    "        q3 = points.rolling(window,min_periods=window//2, center=True).quantile(0.75)\n",
    "        \n",
    "    else:\n",
    "        q1X = points.quantile(0.25)\n",
    "        q3X = points.quantile(0.75)\n",
    "        q1 = points.copy()\n",
    "        q3 = points.copy()\n",
    "        q1.iloc[:] = q1X       \n",
    "        q3.iloc[:] = q3X  \n",
    "        \n",
    "        \n",
    "    IQR = q3 - q1\n",
    "    lower_range = q1 - (t0 * IQR)\n",
    "    upper_range = q3 + (t0 * IQR)   \n",
    "              \n",
    "    IQRmask = ~points.between(lower_range, upper_range)    \n",
    "    IQRmask[:window] = False\n",
    "    \n",
    "    outliers = points[IQRmask]  \n",
    "          \n",
    "    d = {'IQR': IQR, 'outliers': outliers, 'IQRmask': IQRmask, 'lower_range': lower_range, 'upper_range': upper_range }\n",
    "    df = pd.DataFrame(data=d)           \n",
    "    \n",
    "    return IQR, outliers, IQRmask, lower_range, upper_range, df\n",
    "\n",
    "\n",
    "def z_score_filter2(points, window= 0, t0=3.5):\n",
    "    '''\n",
    "    Calculates the z-score for each measurement assuming a normal distribution of values around the mean\n",
    "    Then classifies it as an outlier based on the threshold value\n",
    "    pandas: pandas series of values from which to remove outliers\n",
    "    window: size of window (including the sample; 7 is equal to 3 on either side of value)\n",
    "    t0: threshold\n",
    "    '''\n",
    "    # Make copy so original not edited\n",
    "    vals = points.copy()    \n",
    "   \n",
    "    if window !=0:\n",
    "        rolling_mean = vals.rolling(window,min_periods=window // 2,center=True).mean()\n",
    "        rolling_std = vals.rolling(window,min_periods=window // 2,center=True).std()        \n",
    "        zscore = (vals - rolling_mean)/rolling_std\n",
    "        \n",
    "    else:\n",
    "        rolling_mean = vals.mean()\n",
    "        rolling_std = vals.std()\n",
    "        zscore = (vals - rolling_mean)/rolling_std              \n",
    "    \n",
    "    #outlier_idx = np.abs(zscore > t0)    \n",
    "    outlier_idx = ~((zscore > -t0) & (zscore < t0))    \n",
    "\n",
    "    mean_plus_std = rolling_mean + t0*rolling_std\n",
    "    mean_minus_std = rolling_mean - t0*rolling_std \n",
    "    \n",
    "    outliers = points[outlier_idx]\n",
    "           \n",
    "    return outliers, zscore, rolling_mean, rolling_std,  mean_plus_std, mean_minus_std, outlier_idx\n",
    "\n",
    "\n",
    "\n",
    "def mod_z_score_filter(points, window= 0, t0=3.5):\n",
    "    '''\n",
    "    Calculates the modified z-score for each measurement. It assumes the 0.75th quartile of the standard normal distribution,\n",
    "    to which the MAD converges to.\n",
    "    \n",
    "    '''\n",
    "    # Make copy so original not edited\n",
    "    vals = points.copy()    \n",
    "   \n",
    "    if window !=0:\n",
    "        rolling_median = vals.rolling(window,min_periods=window // 2,center=True).median()\n",
    "        difference=np.abs(rolling_median-vals)\n",
    "        median_abs_deviation=difference.rolling(window,min_periods=window // 2,center=True).median()        \n",
    "        mod_zscore =  0.6745*(vals - rolling_median)/median_abs_deviation                       \n",
    "         \n",
    "    else:\n",
    "        rolling_median = vals.median()\n",
    "        difference=np.abs(rolling_median-vals)\n",
    "        median_abs_deviation=difference.median()       \n",
    "        mod_zscore =  0.6745*(vals - rolling_median)/median_abs_deviation            \n",
    "    \n",
    "       \n",
    "    outlier_idx = ~((mod_zscore > -t0) & (mod_zscore < t0))    \n",
    "\n",
    "    median_plus_std = rolling_median + t0*median_abs_deviation\n",
    "    median_minus_std = rolling_median - t0*median_abs_deviation \n",
    "    \n",
    "    outliers = points[outlier_idx]\n",
    "           \n",
    "    return outliers, mod_zscore, rolling_median, median_abs_deviation,  median_plus_std, median_minus_std, outlier_idx\n",
    "\n",
    "\n",
    "def EWMA(points,window_size = 0, nstd = 3):\n",
    "    \n",
    "    #window_size --> size of moving window\n",
    "    #nstd -->  #number of standard deviations to consider for the boundaries\n",
    "    \n",
    "    \n",
    "    mean = points.ewm(halflife=window_size, adjust=False).mean()\n",
    "    std = points.ewm(halflife=window_size, adjust=False).std()\n",
    "    std.iloc[0] = 0 #the first value turns into NaN because of no data\n",
    "        \n",
    "    mean_plus_std = mean + nstd*std\n",
    "    mean_minus_std = mean - nstd*std      \n",
    "             \n",
    "    is_outlier = (points.values > mean_plus_std) | (points.values < mean_minus_std)\n",
    "    outliers = points[is_outlier]\n",
    "              \n",
    "    return outliers, mean, mean_plus_std,mean_minus_std, is_outlier\n",
    "                \n",
    "\n",
    "def constvalue(points,length,eps=0):\n",
    "    '''\n",
    "    consequitve points are classified as outliers if they dont change more than a defined minimal\n",
    "    among withi a defined window\n",
    "    \n",
    "    eps : minimal amount of change necessary to not be counted as stationary\n",
    "    length : minimal amount of measurements in a row \n",
    "    '''\n",
    "    current_length = 0\n",
    "    data = points.copy()\n",
    "    data.iloc[:] = False\n",
    "    list_of_stationaries = []       \n",
    "    \n",
    "    for i in range(1,len(data)):\n",
    "        current_value = points.Value.iloc[i]\n",
    "        past_value = points.Value.iloc[i-1]\n",
    "                        \n",
    "        if abs(current_value-past_value) <= eps:            \n",
    "            current_length += 1\n",
    "            #display('current_length:',current_length)\n",
    "            if current_length == length:\n",
    "                for measurement in list_of_stationaries:\n",
    "                    data.iloc[measurement] = True\n",
    "                list_of_stationaries = []\n",
    "            if current_length >= length:                \n",
    "                data.iloc[i] = True\n",
    "            else:\n",
    "                list_of_stationaries.append(i)\n",
    "                \n",
    "        else:\n",
    "            current_length = 0\n",
    "            list_of_stationaries = []\n",
    "            \n",
    "    outliers = points[data]    \n",
    "    mask_stationaries =  data   \n",
    "    \n",
    "    Conf_flatline =  mask_stationaries.copy()  \n",
    "    Conf_flatline.replace(False,100, inplace=True)         \n",
    "    Conf_flatline.replace(True,0, inplace=True)          \n",
    "    \n",
    "    #Conf_flatline.to_csv(\"Conf_flatline.csv\",index=False, encoding='utf-8-sig')\n",
    "      \n",
    "    return outliers, mask_stationaries, Conf_flatline\n",
    "\n",
    "\n",
    "def roc2(points,periods, minv,maxv):\n",
    "    '''\n",
    "    point classified as an outlier when the rate of change (roc) between two consecutive points or in a specified period\n",
    "    exceeds the allowed band\n",
    "    '''        \n",
    "    \n",
    "    period_roc = periods\n",
    "    \n",
    "    min_roc = minv    \n",
    "    max_roc = maxv\n",
    "    \n",
    "    #roc = points.pct_change(period_roc,fill_method ='ffill')\n",
    "    roc = 100*points.pct_change(period_roc)  #units in %          \n",
    "    \n",
    "    #List of outliers\n",
    "    is_outlier = ~((roc.Value > min_roc) & (roc.Value < max_roc))\n",
    "    outliers = points[is_outlier]  \n",
    "    \n",
    "    is_outlier = is_outlier.to_frame()   #Convert series to dataframe\n",
    "            \n",
    "    return outliers, roc, is_outlier\n",
    "\n",
    "\n",
    "#Test to detect exceeding values from a defined range \n",
    "def physicalrange(points,minv,maxv):\n",
    "    '''\n",
    "    point classified as an outlier when the value exceeds the allowed band\n",
    "    '''     \n",
    "\n",
    "    min_range = minv\n",
    "    max_range = maxv\n",
    "      \n",
    "    is_outlier = ~((points.Value > min_range) & (points.Value < max_range))\n",
    "    outliers = points[is_outlier]\n",
    "         \n",
    "    is_outlier = is_outlier.to_frame()   #Convert series to dataframe\n",
    "    \n",
    "    return outliers, is_outlier\n",
    "\n",
    "\n",
    "\n",
    "def DataInspection(points,variable):\n",
    "    '''\n",
    "    basic data visualization, data distribution and skewness\n",
    "    '''        \n",
    "    \n",
    "    #Plot time serie, density plot (to see distribution) and box plot (prescense of outliers)\n",
    "    points[variable].iplot(kind='scatter',filename='cufflinks/simple-scatter',mode='lines', size=4,legend=True)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2,figsize=(15, 3))\n",
    "\n",
    "    # Density plot\n",
    "    sns.kdeplot(ax=axes[0],data=points[variable], shade=True)\n",
    "    axes[0].set_title('density plot')\n",
    "\n",
    "    #Box plot\n",
    "    sns.boxplot(ax=axes[1],x=points[variable])\n",
    "    axes[1].set_title('box plot')   \n",
    "    \n",
    "    #Calculation of sumary statistics and Skewness \n",
    "    statistics = points[variable].describe()           \n",
    "    statistics.loc[len(statistics.index)] = [points[variable].skew()]    \n",
    "    statistics.rename(index={8:'skewness'},inplace=True)\n",
    "              \n",
    "    return statistics   \n",
    "\n",
    "\n",
    "def MethodEvaluation(realoutliers, outliersmethod):\n",
    "\n",
    "# Evaluation of the methods. Anomalies as flagged as '1'\n",
    "\n",
    "    cm_test = confusion_matrix(realoutliers, outliersmethod)                     \n",
    "\n",
    "    #proportion of predictions that the model classified correctly (good for symmetric datasets)\n",
    "    accuracy = accuracy_score(realoutliers, outliersmethod)\n",
    "\n",
    "    #proportion of actual positives that was identified correctly (true possitive rate - TPR)\n",
    "    recall = recall_score(realoutliers, outliersmethod, average=None)\n",
    "\n",
    "    #proportion of positive identifications that was actually correct\n",
    "    precision = precision_score(realoutliers, outliersmethod, average=None)\n",
    "\n",
    "    #harmonic mean of precision and recall\n",
    "    f1 = f1_score(realoutliers, outliersmethod, average=None)\n",
    "\n",
    "    ######### Manual calculation #############\n",
    "\n",
    "    #True negative:  the method correctly predicts the negative class (non-anomalous data as non-anomalous)\n",
    "    TN = cm_test[0][0]\n",
    "\n",
    "    #False negative, type 2 error, the method incorrectly predicts the negative class (non-anomalous) when it is actually positive (anomalous)\n",
    "    FN  = cm_test[1][0]\n",
    "\n",
    "    #True positive: methods correctly predicts the positive class (anomalous data as anomalous)\n",
    "    TP  = cm_test[1][1] \n",
    "\n",
    "    #False positive. type 1 error, the method incorrectly predicts the positive class (anomalous) when it is actually negative (non-anomalous)\n",
    "    FP  = cm_test[0][1] \n",
    "\n",
    "    # Sensitivity, hit rate, recall, or true positive rate --> What proportion of actual positives was identified correctly\n",
    "    TPR = TP/(TP+FN)\n",
    "    # Specificity or true negative rate --> proportion of actual negatives that are correctly identified\n",
    "    TNR = TN/(TN+FP) \n",
    "    # Precision or positive predictive value --> What proportion of positive identifications was actually correct\n",
    "    PPV = TP/(TP+FP)\n",
    "    # Negative predictive value\n",
    "    NPV = TN/(TN+FN)\n",
    "    # Fall out or false positive rate\n",
    "    FPR = FP/(FP+TN)\n",
    "    # False negative rate\n",
    "    FNR = FN/(TP+FN)\n",
    "    # False discovery rate\n",
    "    FDR = FP/(TP+FP)\n",
    "    # f1\n",
    "    f1 = 2 * (PPV * TPR) / (PPV + TPR)\n",
    "    # Overall accuracy\n",
    "    ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "\n",
    "    #print(\"recall: \",\"%.2f\" % TPR,\"\\nspecificity: \", \"%.2f\" % TNR,\"\\nprecision: \",\"%.2f\" % PPV, \"\\nFNR: \" ,\"%.2f\" % FNR)\n",
    "\n",
    "    return cm_test, TN, FN, TP, FP, f1, ACC \n",
    "\n",
    "\n",
    "\n",
    "def MethodEvaluationAll(realoutliers, outliersmethod):\n",
    "\n",
    "# Evaluation of the methods. Anomalies as flagged as '1'\n",
    "\n",
    "    cm_test = confusion_matrix(realoutliers, outliersmethod)\n",
    "    \n",
    "    display(cm_test)\n",
    "    \n",
    "    #Printing confusion matrix\n",
    "    group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
    "    group_counts = ['{0:0.0f}'.format(value) for value in\n",
    "                cm_test.flatten()]\n",
    "    group_percentages = ['{0:.2%}'.format(value) for value in\n",
    "                     cm_test.flatten()/np.sum(cm_test)]\n",
    "    labels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in\n",
    "          zip(group_names,group_counts,group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "    sns.heatmap(cm_test, annot=labels, fmt='', cmap='Blues')\n",
    "    plt.xlabel(\"predicted\")\n",
    "    plt.ylabel(\"actual\")\n",
    "         \n",
    "    print (classification_report(realoutliers, outliersmethod))  \n",
    "\n",
    "    #proportion of predictions that the model classified correctly (good for symmetric datasets)\n",
    "    accuracy = accuracy_score(realoutliers, outliersmethod)\n",
    "\n",
    "    #proportion of actual positives that was identified correctly (true possitive rate - TPR)\n",
    "    recall = recall_score(realoutliers, outliersmethod, average=None)\n",
    "\n",
    "    #proportion of positive identifications that was actually correct\n",
    "    precision = precision_score(realoutliers, outliersmethod, average=None)\n",
    "\n",
    "    #harmonic mean of precision and recall\n",
    "    f1 = f1_score(realoutliers, outliersmethod, average=None)\n",
    "\n",
    "    ######### Manual calculation #############\n",
    "\n",
    "    #True negative:  the method correctly predicts the negative class (non-anomalous data as non-anomalous)\n",
    "    TN = cm_test[0][0]\n",
    "\n",
    "    #False negative, type 2 error, the method incorrectly predicts the negative class (non-anomalous) when it is actually positive (anomalous)\n",
    "    FN  = cm_test[1][0]\n",
    "\n",
    "    #True positive: methods correctly predicts the positive class (anomalous data as anomalous)\n",
    "    TP  = cm_test[1][1] \n",
    "\n",
    "    #False positive. type 1 error, the method incorrectly predicts the positive class (anomalous) when it is actually negative (non-anomalous)\n",
    "    FP  = cm_test[0][1] \n",
    "\n",
    "    # Sensitivity, hit rate, recall, or true positive rate --> What proportion of actual positives was identified correctly\n",
    "    TPR = TP/(TP+FN)\n",
    "    # Specificity or true negative rate --> proportion of actual negatives that are correctly identified\n",
    "    TNR = TN/(TN+FP) \n",
    "    # Precision or positive predictive value --> What proportion of positive identifications was actually correct\n",
    "    PPV = TP/(TP+FP)\n",
    "    # Negative predictive value\n",
    "    NPV = TN/(TN+FN)\n",
    "    # Fall out or false positive rate\n",
    "    FPR = FP/(FP+TN)\n",
    "    # False negative rate\n",
    "    FNR = FN/(TP+FN)\n",
    "    # False discovery rate\n",
    "    FDR = FP/(TP+FP)\n",
    "    # f1\n",
    "    f1 = 2 * (PPV * TPR) / (PPV + TPR)\n",
    "    # Overall accuracy\n",
    "    ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "\n",
    "    print(\"recall: \",\"%.2f\" % TPR,\"\\nspecificity: \", \"%.2f\" % TNR,\"\\nprecision: \",\"%.2f\" % PPV, \"\\nFNR: \" ,\"%.2f\" % FNR)\n",
    "\n",
    "    return cm_test \n",
    "\n",
    "\n",
    "\n",
    "def resampling(points, freq, mod):       \n",
    "    \n",
    "    '''\n",
    "    Downsampling — Resample to a wider time frame. It can use aggregation functions: \n",
    "      mean(), min(), max(), sum(), last()-->the last notnull value\n",
    "    Upsampling — Resample to a shorter time frame. New rows can be filled out with: \n",
    "      ffill() --> Use the last known value to fill the new one\n",
    "      bfill() --> Use the next known value to fill the new one \n",
    "    '''\n",
    "\n",
    "    resampMode = mod\n",
    "    base = freq\n",
    "    \n",
    "    #Check first for duplicated data    \n",
    "    points = points[~points.index.duplicated(keep='first')]\n",
    "    \n",
    "    #Resampling of the data    \n",
    "    if resampMode == 'last':\n",
    "        data_res = points.resample(base).last()\n",
    "        \n",
    "    elif resampMode == 'bfill':     \n",
    "        data_res = points.resample(base).bfill()\n",
    "        \n",
    "    elif resampMode == 'ffill':     \n",
    "        data_res = points.resample(base).ffill()  \n",
    "        \n",
    "    elif resampMode == 'mean':     \n",
    "        data_res = points.resample(base).mean() \n",
    "        \n",
    "    elif resampMode == 'min':     \n",
    "        data_res = points.resample(base).min()\n",
    "        \n",
    "    elif resampMode == 'max':     \n",
    "        data_res = points.resample(base).max()\n",
    "        \n",
    "    elif resampMode == 'sum':     \n",
    "        data_res = points.resample(base).sum() \n",
    "               \n",
    "    elif resampMode == 'default':     \n",
    "        data_res = points.resample(base) \n",
    "                                   \n",
    "    return data_res\n",
    "\n",
    "\n",
    "#Functions for ARIMA/ARMA calculations\n",
    "\n",
    "#Test stationarity of the data using the Augmented Dickey-Fuller (ADF) test\n",
    "def test_stationarity(ts_data, column='', signif=0.05, series=False):\n",
    "    if series:\n",
    "        adf_test = adfuller(ts_data, autolag='AIC')\n",
    "    else:\n",
    "        adf_test = adfuller(ts_data[column], autolag='AIC')\n",
    "    p_value = adf_test[1]\n",
    "    if p_value <= signif:\n",
    "        test_result = \"Stationary\"\n",
    "    else:\n",
    "        test_result = \"Non-Stationary\"\n",
    "        \n",
    "    return test_result\n",
    "\n",
    "#Conver data into stationary\n",
    "def differencing(data, column, order):\n",
    "    differenced_data = data[column].diff(order)\n",
    "    differenced_data.fillna(differenced_data.mean(), inplace=True)\n",
    "    \n",
    "    return differenced_data\n",
    "\n",
    "def get_order_sets(n, n_per_set) -> list:\n",
    "    n_sets = [i for i in range(n)]\n",
    "    order_sets = [\n",
    "        n_sets[i:i + n_per_set]\n",
    "        for i in range(0, n, n_per_set)]\n",
    "    \n",
    "    return order_sets\n",
    "\n",
    "def find_aic_for_model(data, p, q, model, model_name):\n",
    "    try:\n",
    "        msg = f\"Fitting {model_name} with order p, q = {p, q}n\"\n",
    "        print(msg)\n",
    "        if p == 0 and q == 1:\n",
    "            # since p=0 and q=1 is already\n",
    "            # calculated\n",
    "            return None, (p, q)\n",
    "        ts_results = model(data, order=(p, q)).fit(disp=False)\n",
    "        curr_aic = ts_results.aic\n",
    "        return curr_aic, (p, q)\n",
    "    except Exception as e:\n",
    "        f\"\"\"Exception occurred continuing {e}\"\"\"\n",
    "        return None, (p, q)\n",
    "\n",
    "#Best order is found using the minimum AIC value for the model\n",
    "def find_best_order_for_model(data, model, model_name):\n",
    "    p_ar, q_ma = max_p, max_q\n",
    "    final_results = []\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    ts_results = model(data, order=(0, 1)).fit(disp=False)    \n",
    "    min_aic = ts_results.aic\n",
    "    final_results.append((min_aic, (0, 1)))\n",
    "    # example if q_ma is 6\n",
    "    # order_sets would be [[0, 1, 2, 3, 4], [5]]\n",
    "    order_sets = get_order_sets(q_ma, 5)\n",
    "    for p in range(0, p_ar):\n",
    "        for order_set in order_sets:\n",
    "            # fit the model and find the aic\n",
    "            results = Parallel(n_jobs=len(order_set), prefer='threads')(\n",
    "                delayed(find_aic_for_model)(data, p, q, model, model_name)\n",
    "                for q in order_set\n",
    "            )\n",
    "            final_results.extend(results)\n",
    "    results_df = pd.DataFrame(final_results,columns=['aic', 'order'])\n",
    "    \n",
    "    min_df = results_df[results_df['aic'] == results_df['aic'].min()]\n",
    "    min_aic = min_df['aic'].iloc[0]\n",
    "    min_order = min_df['order'].iloc[0]\n",
    "    \n",
    "    return min_aic, min_order, results_df\n",
    "\n",
    "#Find anomalies for ARIMA/ARMA model\n",
    "def find_anomalies(squared_errors,z):\n",
    "    threshold = np.mean(squared_errors) + z*np.std(squared_errors)\n",
    "    predictions = (squared_errors >= threshold).astype(int)\n",
    "    return predictions, threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec55336",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
